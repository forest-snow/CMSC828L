/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
[[[[0.09019608 0.05098039 0.05490196]
   [0.08627451 0.04705882 0.05098039]
   [0.09019608 0.04705882 0.05490196]
   ...
   [0.08627451 0.07058824 0.0627451 ]
   [0.09019608 0.07058824 0.06666667]
   [0.08627451 0.07058824 0.0627451 ]]

  [[0.07843137 0.04705882 0.05882353]
   [0.08627451 0.04705882 0.05882353]
   [0.09019608 0.05098039 0.05882353]
   ...
   [0.09019608 0.0745098  0.06666667]
   [0.09019608 0.07058824 0.06666667]
   [0.08235294 0.07058824 0.0627451 ]]

  [[0.07843137 0.04313725 0.05490196]
   [0.08235294 0.04705882 0.05882353]
   [0.08627451 0.05098039 0.05490196]
   ...
   [0.09411765 0.0745098  0.0627451 ]
   [0.08627451 0.07058824 0.05882353]
   [0.08627451 0.07058824 0.0627451 ]]

  ...

  [[0.25490196 0.30980392 0.18431373]
   [0.24313725 0.30196078 0.15686275]
   [0.24705882 0.30196078 0.16078431]
   ...
   [0.13333333 0.16862745 0.10980392]
   [0.1254902  0.15686275 0.09411765]
   [0.14117647 0.17254902 0.11372549]]

  [[0.2627451  0.32156863 0.17254902]
   [0.25882353 0.31764706 0.15294118]
   [0.28235294 0.3372549  0.16862745]
   ...
   [0.1372549  0.18431373 0.11764706]
   [0.13333333 0.18431373 0.10980392]
   [0.14901961 0.19607843 0.12941176]]

  [[0.2627451  0.30980392 0.16470588]
   [0.27843137 0.30980392 0.16470588]
   [0.30588235 0.3254902  0.17647059]
   ...
   [0.14117647 0.19215686 0.11764706]
   [0.14901961 0.20392157 0.12156863]
   [0.16078431 0.21176471 0.14117647]]]


 [[[0.45882353 0.12156863 0.10196078]
   [0.54901961 0.29019608 0.24313725]
   [0.62352941 0.60784314 0.60784314]
   ...
   [0.68627451 0.09411765 0.11372549]
   [0.7372549  0.10196078 0.14901961]
   [0.8745098  0.38039216 0.37647059]]

  [[0.56862745 0.04705882 0.03921569]
   [0.56862745 0.09803922 0.07843137]
   [0.60784314 0.51764706 0.48627451]
   ...
   [0.65098039 0.08627451 0.09411765]
   [0.53333333 0.1372549  0.14901961]
   [0.74901961 0.17647059 0.21960784]]

  [[0.56470588 0.04313725 0.05490196]
   [0.57254902 0.05882353 0.0627451 ]
   [0.51372549 0.29411765 0.23529412]
   ...
   [0.5372549  0.10980392 0.10588235]
   [0.21960784 0.16078431 0.11764706]
   [0.51764706 0.34901961 0.34509804]]

  ...

  [[0.4745098  0.46666667 0.43137255]
   [0.3254902  0.42352941 0.29411765]
   [0.30196078 0.41176471 0.27843137]
   ...
   [0.0745098  0.09019608 0.03137255]
   [0.14117647 0.09803922 0.03137255]
   [0.19215686 0.10588235 0.04313725]]

  [[0.41960784 0.43921569 0.36862745]
   [0.30196078 0.41176471 0.28627451]
   [0.25098039 0.37254902 0.23137255]
   ...
   [0.03529412 0.03921569 0.02352941]
   [0.11372549 0.0745098  0.03529412]
   [0.29411765 0.15294118 0.05882353]]

  [[0.38823529 0.4        0.34901961]
   [0.24313725 0.34901961 0.21960784]
   [0.19215686 0.31372549 0.15686275]
   ...
   [0.05882353 0.05098039 0.02352941]
   [0.1372549  0.09411765 0.03529412]
   [0.28627451 0.1372549  0.05098039]]]


 [[[0.10588235 0.08235294 0.00392157]
   [0.14901961 0.10980392 0.00392157]
   [0.12941176 0.09803922 0.00784314]
   ...
   [0.14509804 0.10980392 0.49411765]
   [0.15686275 0.10980392 0.4745098 ]
   [0.17254902 0.10980392 0.45882353]]

  [[0.23137255 0.20392157 0.00784314]
   [0.28627451 0.25490196 0.00784314]
   [0.28235294 0.25882353 0.00784314]
   ...
   [0.14509804 0.1254902  0.51372549]
   [0.16470588 0.12941176 0.50588235]
   [0.17647059 0.1254902  0.48627451]]

  [[0.35294118 0.3254902  0.00392157]
   [0.40784314 0.38823529 0.00392157]
   [0.43921569 0.42352941 0.00392157]
   ...
   [0.14117647 0.13333333 0.49019608]
   [0.16078431 0.13333333 0.50196078]
   [0.17647059 0.12941176 0.49019608]]

  ...

  [[0.82745098 0.70980392 0.03529412]
   [0.78039216 0.63137255 0.03529412]
   [0.79607843 0.52941176 0.03529412]
   ...
   [0.78823529 0.72941176 0.01176471]
   [0.77647059 0.72156863 0.01176471]
   [0.74901961 0.69019608 0.01176471]]

  [[0.83921569 0.82745098 0.03529412]
   [0.79607843 0.62745098 0.03921569]
   [0.76862745 0.43529412 0.03137255]
   ...
   [0.82352941 0.79215686 0.00784314]
   [0.81568627 0.78823529 0.00784314]
   [0.81960784 0.8        0.00784314]]

  [[0.88627451 0.85098039 0.00784314]
   [0.80392157 0.5372549  0.00784314]
   [0.75686275 0.36078431 0.00784314]
   ...
   [0.89803922 0.91764706 0.00784314]
   [0.88627451 0.90980392 0.00784314]
   [0.8627451  0.8745098  0.00784314]]]


 ...


 [[[0.56078431 0.6745098  0.81568627]
   [0.51764706 0.64313725 0.79215686]
   [0.52156863 0.64705882 0.79607843]
   ...
   [0.5254902  0.65098039 0.8       ]
   [0.52156863 0.64705882 0.79607843]
   [0.56470588 0.68627451 0.81960784]]

  [[0.56078431 0.6745098  0.81568627]
   [0.52156863 0.65098039 0.8       ]
   [0.5254902  0.65098039 0.8       ]
   ...
   [0.52941176 0.65490196 0.80392157]
   [0.50588235 0.62352941 0.74509804]
   [0.51764706 0.60784314 0.64313725]]

  [[0.56862745 0.67843137 0.82352941]
   [0.52941176 0.65490196 0.80392157]
   [0.53333333 0.65882353 0.80784314]
   ...
   [0.4627451  0.56078431 0.62352941]
   [0.30980392 0.34509804 0.20392157]
   [0.36470588 0.40392157 0.11764706]]

  ...

  [[0.57254902 0.69019608 0.82352941]
   [0.5372549  0.6627451  0.81176471]
   [0.54117647 0.66666667 0.82352941]
   ...
   [0.54509804 0.67058824 0.83137255]
   [0.5372549  0.6627451  0.82352941]
   [0.58039216 0.69411765 0.83529412]]

  [[0.57254902 0.68627451 0.82745098]
   [0.53333333 0.65882353 0.80784314]
   [0.54117647 0.66666667 0.81568627]
   ...
   [0.54117647 0.66666667 0.82745098]
   [0.5372549  0.6627451  0.82352941]
   [0.58039216 0.69411765 0.83529412]]

  [[0.57254902 0.68627451 0.82745098]
   [0.53333333 0.65882353 0.80784314]
   [0.5372549  0.6627451  0.81176471]
   ...
   [0.54117647 0.66666667 0.82352941]
   [0.53333333 0.65882353 0.81960784]
   [0.58039216 0.69411765 0.83529412]]]


 [[[0.10980392 0.10196078 0.28627451]
   [0.10980392 0.10196078 0.28627451]
   [0.11372549 0.10588235 0.29019608]
   ...
   [0.10588235 0.09411765 0.28235294]
   [0.10980392 0.09803922 0.28235294]
   [0.10980392 0.10196078 0.27843137]]

  [[0.11764706 0.11372549 0.29411765]
   [0.1254902  0.11764706 0.29803922]
   [0.1254902  0.11764706 0.30196078]
   ...
   [0.07843137 0.07058824 0.25882353]
   [0.07843137 0.07058824 0.25490196]
   [0.07843137 0.07058824 0.25490196]]

  [[0.16470588 0.16078431 0.32156863]
   [0.16078431 0.16078431 0.32156863]
   [0.15686275 0.15686275 0.3254902 ]
   ...
   [0.0745098  0.07058824 0.25882353]
   [0.07058824 0.06666667 0.25098039]
   [0.07058824 0.0627451  0.24313725]]

  ...

  [[0.35294118 0.45882353 0.20784314]
   [0.36862745 0.46666667 0.18823529]
   [0.39607843 0.46666667 0.21176471]
   ...
   [0.33333333 0.42352941 0.19607843]
   [0.37254902 0.49019608 0.1254902 ]
   [0.3372549  0.43921569 0.17254902]]

  [[0.34509804 0.43529412 0.22745098]
   [0.34509804 0.44705882 0.19215686]
   [0.36862745 0.45490196 0.21176471]
   ...
   [0.34901961 0.43921569 0.20392157]
   [0.34509804 0.44705882 0.16470588]
   [0.34509804 0.43921569 0.18039216]]

  [[0.33333333 0.41960784 0.23921569]
   [0.34117647 0.43921569 0.20784314]
   [0.34509804 0.44705882 0.20784314]
   ...
   [0.35294118 0.44705882 0.21960784]
   [0.34117647 0.44313725 0.18431373]
   [0.35294118 0.44313725 0.2       ]]]


 [[[0.27058824 0.18431373 0.05098039]
   [0.41568627 0.2745098  0.10196078]
   [0.59215686 0.38823529 0.16470588]
   ...
   [0.84313725 0.61960784 0.4745098 ]
   [0.85490196 0.61568627 0.43921569]
   [0.93333333 0.67058824 0.43921569]]

  [[0.38823529 0.2627451  0.09411765]
   [0.60392157 0.39215686 0.16862745]
   [0.78039216 0.50588235 0.23137255]
   ...
   [0.76470588 0.55294118 0.41960784]
   [0.75294118 0.54509804 0.4       ]
   [0.84313725 0.60392157 0.42352941]]

  [[0.54509804 0.36078431 0.14509804]
   [0.7372549  0.47843137 0.21568627]
   [0.88627451 0.58039216 0.28235294]
   ...
   [0.70588235 0.50588235 0.35686275]
   [0.78823529 0.58431373 0.38823529]
   [0.83529412 0.60392157 0.41176471]]

  ...

  [[0.4        0.30196078 0.25882353]
   [0.38039216 0.28627451 0.23921569]
   [0.36862745 0.27058824 0.21568627]
   ...
   [0.03529412 0.01568627 0.00784314]
   [0.05098039 0.02352941 0.01176471]
   [0.04705882 0.01960784 0.01176471]]

  [[0.31372549 0.23137255 0.17254902]
   [0.3372549  0.24313725 0.16470588]
   [0.35686275 0.24313725 0.1254902 ]
   ...
   [0.10980392 0.0627451  0.03921569]
   [0.09411765 0.04705882 0.03137255]
   [0.06666667 0.03137255 0.01960784]]

  [[0.31764706 0.22352941 0.13333333]
   [0.30588235 0.21176471 0.11372549]
   [0.31764706 0.21960784 0.10980392]
   ...
   [0.05882353 0.03137255 0.01960784]
   [0.03529412 0.01568627 0.00784314]
   [0.02352941 0.00784314 0.00784314]]]]
[[0. 0. 0. 0. 1.]
 [0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0.]
 ...
 [0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0.]]
Traceback (most recent call last):
  File "model_flowers.py", line 170, in <module>
    train_acc = train()
  File "model_flowers.py", line 134, in train
    outputs = model(images)
  File "/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "model_flowers.py", line 112, in forward
    output = self.net(input)
  File "/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py", line 91, in forward
    input = module(input)
  File "/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "model_flowers.py", line 82, in forward
    output = self.conv(input)
  File "/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/cliphomes/myuan/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same
